{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-24T17:21:21.364891Z","iopub.status.busy":"2024-08-24T17:21:21.364478Z","iopub.status.idle":"2024-08-24T17:21:21.787690Z","shell.execute_reply":"2024-08-24T17:21:21.786629Z","shell.execute_reply.started":"2024-08-24T17:21:21.364840Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/oniondata/weekly_wholesale_price_onion-upto_2012_1.csv\n","/kaggle/input/potatodataset/weekly_wholesale_price_potato-upto_2012.csv\n","/kaggle/input/ricedataset/weekly_wholesale_price_rice-upto_2012.csv\n","/kaggle/input/weekly-year-wise/weekly_wholesale_price_wheat-upto_2012.csv\n","/kaggle/input/milkdata/weekly_wholesale_price_milk-upto_2012.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T17:38:40.047852Z","iopub.status.busy":"2024-08-24T17:38:40.047455Z","iopub.status.idle":"2024-08-24T17:38:40.513880Z","shell.execute_reply":"2024-08-24T17:38:40.512828Z","shell.execute_reply.started":"2024-08-24T17:38:40.047809Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T17:21:26.988796Z","iopub.status.busy":"2024-08-24T17:21:26.988371Z","iopub.status.idle":"2024-08-24T17:21:26.995379Z","shell.execute_reply":"2024-08-24T17:21:26.994002Z","shell.execute_reply.started":"2024-08-24T17:21:26.988746Z"},"trusted":true},"outputs":[],"source":["# import pandas as pd\n","\n","# # Function to process and save the top k locations for a given crop\n","# def process_top_k_locations_per_crop(df, crop_name, k=5):\n","#     \"\"\"\n","#     Process the top k locations in the DataFrame for a given crop based on the number of non-missing values,\n","#     and capitalize 'chandigarh' to 'CHANDIGARH' in the 'centre' column.\n","    \n","#     Parameters:\n","#     df (DataFrame): The DataFrame containing the data for one crop.\n","#     crop_name (str): The name of the crop (used for file naming).\n","#     k (int): The number of top locations to return based on non-missing values.\n","\n","#     Returns:\n","#     None\n","#     \"\"\"\n","#     # Capitalize 'chandigarh' to 'CHANDIGARH'\n","#     df.loc[df['centre'] == 'chandigarh', 'centre'] = 'CHANDIGARH'\n","\n","#     # Collect locations with their count of non-missing values\n","#     location_non_missing = []\n","\n","#     # Calculate non-missing values for each location\n","#     for location in df['centre'].unique():\n","#         non_missing_count = df[df['centre'] == location].notna().sum().sum()\n","#         location_non_missing.append((location, non_missing_count))\n","\n","#     # Sort locations based on the number of non-missing values in descending order\n","#     location_non_missing.sort(key=lambda x: x[1], reverse=True)\n","\n","#     # Select the top k locations\n","#     top_k_locations = location_non_missing[:k]\n","\n","#     # Save the filtered data for the top k locations to CSV files\n","#     for location, _ in top_k_locations:\n","#         filtered = df[df['centre'] == location].dropna()\n","#         filtered = filtered[['Date', 'centre', 'Price']]\n","#         filtered.to_csv(f'{crop_name}.csv', index=False)\n","\n","# # Load datasets\n","# wheat_df = pd.read_csv('/kaggle/input/weekly-year-wise/weekly_wholesale_price_wheat-upto_2012.csv')\n","# rice_df = pd.read_csv('/kaggle/input/ricedataset/weekly_wholesale_price_rice-upto_2012.csv')\n","# onion_df = pd.read_csv('/kaggle/input/oniondata/weekly_wholesale_price_onion-upto_2012_1.csv')\n","# potato_df = pd.read_csv('/kaggle/input/potatodataset/weekly_wholesale_price_potato-upto_2012.csv')\n","\n","# # Process and save data for the top 5 locations for each crop\n","# process_top_k_locations_per_crop(wheat_df, \"wheat\", k=5)\n","# process_top_k_locations_per_crop(rice_df, \"rice\", k=5)\n","# process_top_k_locations_per_crop(onion_df, \"onion\", k=5)\n","# process_top_k_locations_per_crop(potato_df, \"potato\", k=5)\n","# #"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T17:39:01.891593Z","iopub.status.busy":"2024-08-24T17:39:01.891038Z","iopub.status.idle":"2024-08-24T17:39:04.253995Z","shell.execute_reply":"2024-08-24T17:39:04.252763Z","shell.execute_reply.started":"2024-08-24T17:39:01.891549Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_36/2788684281.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  df['Date'] = pd.to_datetime(df['Date'])\n","/tmp/ipykernel_36/2788684281.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  df['Date'] = pd.to_datetime(df['Date'])\n","/tmp/ipykernel_36/2788684281.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  df['Date'] = pd.to_datetime(df['Date'])\n","/tmp/ipykernel_36/2788684281.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  df['Date'] = pd.to_datetime(df['Date'])\n"]}],"source":["import pandas as pd\n","\n","def process_and_combine_top_k_locations_per_crop(crop_dfs, k=5):\n","    \"\"\"\n","    Process the top k locations for each crop based on the longest continuous time series,\n","    combine the data for all locations for each crop, and save to CSV files.\n","    \n","    Parameters:\n","    crop_dfs (dict): A dictionary with crop names as keys and DataFrames as values.\n","    k (int): The number of top locations to process for each crop.\n","    \n","    Returns:\n","    None\n","    \"\"\"\n","    for crop_name, df in crop_dfs.items():\n","        # Convert 'Date' to datetime\n","        df['Date'] = pd.to_datetime(df['Date'])\n","        \n","        # Capitalize 'chandigarh' to 'CHANDIGARH'\n","        df.loc[df['centre'] == 'chandigarh', 'centre'] = 'CHANDIGARH'\n","        \n","        # Calculate the longest continuous time series for each location\n","        location_time_series = []\n","        for location in df['centre'].unique():\n","            location_df = df[df['centre'] == location].sort_values('Date')\n","            if not location_df.empty:\n","                date_diffs = location_df['Date'].diff().dt.days\n","                max_continuous = (date_diffs != 7).cumsum().value_counts().max()\n","                location_time_series.append((location, max_continuous))\n","        \n","        # Sort locations based on the length of continuous time series in descending order\n","        location_time_series.sort(key=lambda x: x[1], reverse=True)\n","        \n","        # Select the top k locations\n","        top_k_locations = [loc for loc, _ in location_time_series[:k]]\n","        \n","        # Filter data for top k locations and combine\n","        combined_data = []\n","        for location in top_k_locations:\n","            filtered = df[df['centre'] == location].sort_values('Date')\n","            filtered = filtered[['Date', 'centre', 'Price']]\n","            combined_data.append(filtered)\n","        \n","        # Combine data for all top locations for this crop\n","        if combined_data:\n","            combined_df = pd.concat(combined_data, ignore_index=True)\n","            combined_df = combined_df[['Date', 'centre', 'Price']]\n","            \n","            # Sort the combined data by Date\n","            combined_df = combined_df.sort_values('Date')\n","            combine_df = combined_df.dropna()\n","            # Save to CSV\n","            combined_df.to_csv(f'{crop_name}_top_{k}_locations_timeseries.csv', index=False)\n","\n","# Load datasets\n","crop_dfs = {\n","    'wheat': pd.read_csv('/kaggle/input/weekly-year-wise/weekly_wholesale_price_wheat-upto_2012.csv'),\n","    'rice': pd.read_csv('/kaggle/input/ricedataset/weekly_wholesale_price_rice-upto_2012.csv'),\n","    'onion': pd.read_csv('/kaggle/input/oniondata/weekly_wholesale_price_onion-upto_2012_1.csv'),\n","    'potato': pd.read_csv('/kaggle/input/potatodataset/weekly_wholesale_price_potato-upto_2012.csv')\n","}\n","\n","# Process and save combined data for the top 5 locations for each crop\n","process_and_combine_top_k_locations_per_crop(crop_dfs, k=5)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T17:39:12.357280Z","iopub.status.busy":"2024-08-24T17:39:12.356831Z","iopub.status.idle":"2024-08-24T17:39:13.553692Z","shell.execute_reply":"2024-08-24T17:39:13.552220Z","shell.execute_reply.started":"2024-08-24T17:39:12.357236Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["  adding: kaggle/working/ (stored 0%)\n","  adding: kaggle/working/rice_top_5_locations_timeseries.csv (deflated 85%)\n","  adding: kaggle/working/wheat_top_5_locations_timeseries.csv (deflated 85%)\n","  adding: kaggle/working/onion_top_5_locations_timeseries.csv (deflated 85%)\n","  adding: kaggle/working/potato_top_5_locations_timeseries.csv (deflated 85%)\n","  adding: kaggle/working/.virtual_documents/ (stored 0%)\n"]}],"source":["!zip -r file_combined.zip /kaggle/working"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T17:39:16.692927Z","iopub.status.busy":"2024-08-24T17:39:16.692479Z","iopub.status.idle":"2024-08-24T17:39:16.704185Z","shell.execute_reply":"2024-08-24T17:39:16.702617Z","shell.execute_reply.started":"2024-08-24T17:39:16.692880Z"},"trusted":true},"outputs":[{"data":{"text/html":["<a href='file_combined.zip' target='_blank'>file_combined.zip</a><br>"],"text/plain":["/kaggle/working/file_combined.zip"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from IPython.display import FileLink\n","FileLink(r'file_combined.zip')"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-24T17:37:31.990148Z","iopub.status.busy":"2024-08-24T17:37:31.989686Z","iopub.status.idle":"2024-08-24T17:37:32.396110Z","shell.execute_reply":"2024-08-24T17:37:32.394873Z","shell.execute_reply.started":"2024-08-24T17:37:31.990097Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_36/1679371191.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  df['Date'] = pd.to_datetime(df['Date'])\n"]},{"data":{"text/plain":["Date      0\n","centre    0\n","Price     0\n","dtype: int64"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","def process_and_combine_top_k_locations_per_crop(crop_dfs, k=5):\n","    \"\"\"\n","    Process the top k locations for each crop based on the longest continuous time series,\n","    combine the data for all locations for each crop, and save to CSV files.\n","    \n","    Parameters:\n","    crop_dfs (dict): A dictionary with crop names as keys and DataFrames as values.\n","    k (int): The number of top locations to process for each crop.\n","    \n","    Returns:\n","    None\n","    \"\"\"\n","    for crop_name, df in crop_dfs.items():\n","        # Convert 'Date' to datetime\n","        df['Date'] = pd.to_datetime(df['Date'])\n","        \n","        # Capitalize 'chandigarh' to 'CHANDIGARH'\n","        df.loc[df['centre'] == 'chandigarh', 'centre'] = 'CHANDIGARH'\n","        \n","        # Calculate the longest continuous time series for each location\n","        location_time_series = []\n","        for location in df['centre'].unique():\n","            location_df = df[df['centre'] == location].sort_values('Date')\n","            if not location_df.empty:\n","                date_diffs = location_df['Date'].diff().dt.days\n","                max_continuous = (date_diffs != 7).cumsum().value_counts().max()\n","                location_time_series.append((location, max_continuous))\n","        \n","        # Sort locations based on the length of continuous time series in descending order\n","        location_time_series.sort(key=lambda x: x[1], reverse=True)\n","        \n","        # Select the top k locations\n","        top_k_locations = [loc for loc, _ in location_time_series[:k]]\n","        \n","        # Filter data for top k locations and combine\n","        combined_data = []\n","        for location in top_k_locations:\n","            filtered = df[df['centre'] == location].sort_values('Date')\n","            filtered = filtered[['Date', 'centre', 'Price']]\n","            combined_data.append(filtered)\n","        \n","        # Combine data for all top locations for this crop\n","        if combined_data:\n","            combined_df = pd.concat(combined_data, ignore_index=True)\n","            combined_df = combined_df[['Date', 'centre', 'Price']]\n","            \n","            # Sort the combined data by Date\n","            combined_df = combined_df.sort_values('Date')\n","            combine_df = combined_df.dropna()\n","            # Save to CSV\n","            combined_df.to_csv(f'{crop_name}_top_{k}_locations_timeseries.csv', index=False)\n","\n","# Load datasets\n","crop_dfs = {\n","    'wheat': pd.read_csv('/kaggle/input/weekly-year-wise/weekly_wholesale_price_wheat-upto_2012.csv'),\n","    'rice': pd.read_csv('/kaggle/input/ricedataset/weekly_wholesale_price_rice-upto_2012.csv'),\n","    'onion': pd.read_csv('/kaggle/input/oniondata/weekly_wholesale_price_onion-upto_2012_1.csv'),\n","    'potato': pd.read_csv('/kaggle/input/potatodataset/weekly_wholesale_price_potato-upto_2012.csv')\n","}\n","\n","# Process and save combined data for the top 5 locations for each crop\n","process_and_combine_top_k_locations_per_crop(crop_dfs, k=5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5587968,"sourceId":9238087,"sourceType":"datasetVersion"},{"datasetId":5588648,"sourceId":9239102,"sourceType":"datasetVersion"},{"datasetId":5588657,"sourceId":9239115,"sourceType":"datasetVersion"},{"datasetId":5588760,"sourceId":9239286,"sourceType":"datasetVersion"},{"datasetId":5588789,"sourceId":9239324,"sourceType":"datasetVersion"}],"dockerImageVersionId":30761,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
